"""
Avatar LoRA trainer for Persona Foundry integration.
Wrapper around Kohya/diffusers for SD 1.5 LoRA training.
Supports both CUDA and Apple Silicon MPS.
"""

import argparse
import json
import os
import subprocess
import sys
import platform
from pathlib import Path
from typing import Dict, Any, Optional
import yaml

from .talent_kit import TalentKit
from .config_loader import AvatarConfigLoader


class AvatarTrainer:
    """Handles avatar LoRA training for Persona Foundry."""
    
    def __init__(self, base_dir: Path):
        self.base_dir = Path(base_dir)
        self.talent_kit = TalentKit(base_dir)
        self.config_loader = AvatarConfigLoader(base_dir)
        
        # Platform detection
        self.is_apple_silicon = platform.system() == "Darwin" and platform.machine() == "arm64"
        self.is_cuda_available = self._check_cuda_availability()
        
    def _check_cuda_availability(self) -> bool:
        """Check if CUDA is available."""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _get_training_backend(self) -> str:
        """Determine the best training backend."""
        if self.is_apple_silicon:
            return "mps"
        elif self.is_cuda_available:
            return "cuda"
        else:
            return "cpu"
    
    def _setup_environment(self, backend: str) -> Dict[str, str]:
        """Setup environment variables for training."""
        env = os.environ.copy()
        
        if backend == "mps":
            # Apple Silicon optimizations
            env.update({
                "OMP_NUM_THREADS": "4",
                "PYTORCH_MPS_HIGH_WATERMARK_RATIO": "0.6",
                "PYTORCH_MPS_LOW_WATERMARK_RATIO": "0.4"
            })
        elif backend == "cuda":
            # CUDA optimizations
            env.update({
                "CUDA_VISIBLE_DEVICES": "0",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512"
            })
        
        return env
    
    def _create_training_script(self, config: Dict[str, Any], output_dir: Path) -> Path:
        """Create a training script based on the configuration."""
        backend = self._get_training_backend()
        
        # Basic training parameters
        script_content = f"""#!/usr/bin/env python3
'''
Auto-generated training script for {config['id']}
Generated by Talent Factory Avatar Trainer
'''

import torch
from diffusers import StableDiffusionPipeline, UNet2DConditionModel
from diffusers import DDPMScheduler, DDIMScheduler
from diffusers import AutoencoderKL
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType
import os
from pathlib import Path

# Configuration
MODEL_ID = "{config['base_model']}"
OUTPUT_DIR = Path("{output_dir}")
TRAIN_DATA_DIR = Path("{config['train_data_dir']}")
RESOLUTION = {config['resolution']}
LORA_RANK = {config['lora_rank']}
MAX_TRAIN_STEPS = {config['max_train_steps']}
LEARNING_RATE = {config['learning_rate']}
BATCH_SIZE = {config['batch_size']}
NEGATIVES = "{config.get('negatives', 'photo, text, watermark')}"

# Device setup
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using device: {{device}}")

# Load base model
print("Loading base model...")
pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float16 if device != "cpu" else torch.float32)
pipe = pipe.to(device)

# Setup LoRA
print("Setting up LoRA...")
lora_config = LoraConfig(
    task_type=TaskType.DIFFUSION,
    r=LORA_RANK,
    lora_alpha=LORA_RANK,
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
    lora_dropout=0.1,
)

# Apply LoRA to UNet
pipe.unet = get_peft_model(pipe.unet, lora_config)

# Training setup
optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=LEARNING_RATE)

# Training loop (simplified)
print("Starting training...")
for step in range(MAX_TRAIN_STEPS):
    # TODO: Implement actual training loop with data loading
    if step % 100 == 0:
        print(f"Step {{step}}/{{MAX_TRAIN_STEPS}}")
    
    # Placeholder training step
    pass

# Save LoRA weights
print("Saving LoRA weights...")
pipe.unet.save_pretrained(OUTPUT_DIR / "lora_weights")

print("Training complete!")
"""
        
        script_path = output_dir / "train_script.py"
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        # Make executable
        script_path.chmod(0o755)
        
        return script_path
    
    def train(self, config_path: Path) -> Dict[str, Any]:
        """Train an avatar LoRA based on configuration."""
        print(f"Loading configuration from {config_path}")
        
        # Load and validate config
        config = self.config_loader.load_config(config_path)
        validation = self.config_loader.validate_config(config)
        
        if not validation["valid"]:
            return {
                "success": False,
                "error": f"Configuration validation failed: {validation['errors']}"
            }
        
        talent_id = config["id"]
        print(f"Training talent: {talent_id}")
        
        # Create kit structure
        kit_dir = self.talent_kit.create_kit_structure(talent_id)
        print(f"Created kit directory: {kit_dir}")
        
        # Determine backend
        backend = self._get_training_backend()
        print(f"Using backend: {backend}")
        
        # Setup environment
        env = self._setup_environment(backend)
        
        # Create training script
        script_path = self._create_training_script(config, kit_dir)
        print(f"Created training script: {script_path}")
        
        # For now, create a mock training result
        # TODO: Implement actual training
        print("Starting mock training...")
        
        # Create mock weights file
        weights_path = kit_dir / "weights.safetensors"
        with open(weights_path, 'w') as f:
            f.write("# Mock weights file\n")
        
        # Create base.txt
        base_txt_path = kit_dir / "base.txt"
        with open(base_txt_path, 'w') as f:
            f.write(f"Base model: {config['base_model']}\n")
            f.write(f"SDX version: {config['sdx_version']}\n")
            f.write(f"LoRA rank: {config['lora_rank']}\n")
        
        # Create prompts.json
        prompts_path = kit_dir / "prompts.json"
        prompts_data = {
            "positive_prompts": [
                f"a {config.get('token', 'character')}",
                f"high quality, detailed",
                "best quality, masterpiece"
            ],
            "negative_prompts": config.get('negatives', 'photo, text, watermark').split(', '),
            "style_prompts": [
                "anime style",
                "digital art",
                "character design"
            ]
        }
        with open(prompts_path, 'w') as f:
            json.dump(prompts_data, f, indent=2)
        
        # Create examples directory with placeholder
        examples_dir = kit_dir / "examples"
        examples_dir.mkdir(exist_ok=True)
        placeholder_path = examples_dir / "placeholder.txt"
        with open(placeholder_path, 'w') as f:
            f.write("Example images will be generated here after training\n")
        
        # Create manifest
        manifest = self.talent_kit.create_manifest_template(
            talent_id,
            **config
        )
        
        # Calculate size
        try:
            size_bytes = weights_path.stat().st_size
            manifest["size_mb"] = max(size_bytes / (1024 * 1024), 0.1)
        except:
            manifest["size_mb"] = 0.1
        
        # Save manifest
        self.talent_kit.save_manifest(talent_id, manifest)
        
        print(f"Training complete! Kit created at: {kit_dir}")
        
        return {
            "success": True,
            "talent_id": talent_id,
            "kit_path": str(kit_dir),
            "backend": backend,
            "manifest": manifest
        }


def main():
    parser = argparse.ArgumentParser(description="Train avatar LoRA")
    parser.add_argument("--config", "-c", type=Path, required=True,
                       help="Path to YAML configuration file")
    parser.add_argument("--base-dir", type=Path, default=Path.cwd(),
                       help="Base directory for talent factory")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Verbose output")
    
    args = parser.parse_args()
    
    trainer = AvatarTrainer(args.base_dir)
    
    try:
        result = trainer.train(args.config)
        
        if result["success"]:
            print(f"✅ Training successful!")
            print(f"Talent ID: {result['talent_id']}")
            print(f"Kit path: {result['kit_path']}")
            print(f"Backend: {result['backend']}")
            sys.exit(0)
        else:
            print(f"❌ Training failed: {result['error']}")
            sys.exit(1)
            
    except Exception as e:
        print(f"❌ Training error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
